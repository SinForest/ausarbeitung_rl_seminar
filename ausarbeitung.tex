%% LyX 2.2.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt,english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{babel}
\begin{document}

\section{Abstract}


\section{Markov Entscheidungs-Probleme}

Markov Entscheidungs-Probleme (kurz \textbf{MDP} für \emph{markov
decision process}) sind ein Modell für Probleme, bei denen ein Agent
versucht die größtmögliche Belohnung (\emph{reward}) zu erzielen.
Er bewegt sich dazu durch eine Menge von Zuständen (\emph{states})
indem er aus einer Menge von Aktionen (\emph{actions}) wählt. Welchen
Zustand der Agent erreicht ist nicht deterministisch, die Wahrscheinlichkeiten
sind jedoch nur von der gewählten Aktion und dem aktuellen Zustand
abhängig. Die Belohnung, die der Agent für einen Übergang erhält,
wird durch Ausgangs- und Endzustand des Übergangs so wie die gewählte
Aktion bestimmt. Der Agent startet in einem Startzustand, es kann
diverse Endzustände geben, nach deren Erreichen keine Aktionen mehr
ausgeführt werden können.

Ziel ist es, eine Strategie (\emph{policy}) zu finden, welche jedem
Zustand die Aktion zuordnet, durch die der höchste Gesamtgewinn (also
die höchste Summe über alle erzielten Gewinne) erwartet werden kann.

\subsection{Ein Beispiel}

In diesem Beispiel werden die Zustände durch die Felder eines $4\times4$-Grids
visualisiert. Der Startzustand $\left(3,0\right)$ ist durch ein großes
$S$ markiert, die Zustände mit einem $X$ sind nicht erreichbar.

\includegraphics[scale=0.35]{media/example}

Die Menge der Aktionen besteht aus 4 Elementen, jedes symbolisiert
einen Schritt in eine der vier Himmelsrichtungen. Beim Ausführen einer
Aktion landet der Agent mit einer Chance von $0.8$ ein Feld weiter
in der gewählten Himmelsrichtung, so wie mit je einer Chance von $0.1$
in einer der zwei orthogonalen Richtungen. Würde der Agent hierbei
aif einem Feld landen, welches er nicht betreten kann (eines der mit
$X$ markierten, oder außerhalb des Grids), bleibt er auf seinem Feld.

Von den bunten Feldern aus führt jede Aktion mit einer Chance von
$1$ in den Endzustand. Der Agent erhält für diesen Übergang die auf
dem Feld verzeichnete Belohnung (100 oder -100). Für jede andere Aktion
wird eine Belohnung von -2 verbucht.

\subsection{Notation}

Zur Beschreibung eines MDPs werden einige Notationen benötigt, die
hier angegeben werden. \emph{(Zu einigen Notationen werden Beispiele
zum oben genannten Beispiel angegeben.)} Zur Problemstellung selbst
gehören:
\begin{itemize}
\item $S=\left\{ s_{1},\ldots,s_{n}\right\} $ ist die Menge der \textbf{Zustände}.
Die Elemente der Menge können beliebig benannt werden.
\item $A=\left\{ a_{1},\ldots,a_{n}\right\} $ ist die Menge der \textbf{Aktionen}.
Auch diese Elemente können beliebig benannt werden.
\item $T:S\times A\times S\rightarrow\left[0,1\right]$ ist die \textbf{Übergangs-Funktion}
(\emph{transition function}). $T\left(s,a,s^{\prime}\right)$ beschreibt
die Wahrscheinlichkeit, mit der der Agent von Zustand $s$ in den
Zustand $s^{\prime}$ wechselt, wenn er Aktion $a$ ausführt. Eine
allgemeinere, jedoch hier nicht verwendete Schreibweise wäre $T\left(s,a,s^{\prime}\right)=p\left(s^{\prime}|s,a\right)$.
\begin{align*}
T(s_{0,2},a_{E},s_{0,3}) & =0.8 & \text{(Agent wählt Osten und kommt dort an)}\\
T(s_{0,2},a_{E},s_{0,2}) & =0.1 & \text{(Agent wählt Osten, geht nach Norden, kein valider Zug)}\\
T(s_{0,2},a_{E},s_{1,2}) & =0.1 & \text{(Agent wählt Osten, geht nach Süden)}\\
T(s_{0,2},a_{E},s_{0,1}) & =0.0 & \text{(Agent wählt Osten, Bewegung nach Westen unmöglich)}\\
T(s_{1,2},\_,s_{end}) & =1.0 & \text{(jede Aktion von bunten Feldern führt zum Endzustand)}
\end{align*}
\item $R:S\times A\times S\rightarrow\mathbb{R}$ ist die \textbf{Belohnungs-Funktion}
(\emph{reward function}). $R\left(s,a,s^{\prime}\right)$ ordnet dem
Übergang von Zustand $s$ nach Zustand $s^{\prime}$ mit Hilfe von
Aktion $a$ eine Belohung zu.
\begin{align*}
R(s_{1,2},\_,s_{end}) & =-100 & \text{(Belohnung von buntem Feld)}\\
R(s_{1,3},\_,s_{end}) & =+100 & \text{(Belohnung von buntem Feld)}\\
R(s_{0,2},a_{N},s_{0,1}) & =-2 & \text{(jede Bewegung kostet 2 Belohung)}
\end{align*}
\end{itemize}
Zum Lösen der Probleme werden folgende Notationen verwendet:
\begin{itemize}
\item $\pi\left(s\right)$ ist eine \textbf{Policy}, die dem Zustand $s$
die optimale Aktion zuordnet. Hierbei beschreibt $\pi$ eine Policy
im Allgemeinen, $\pi^{\ast}$ die optimale Policy.
\item $\gamma\in\left[0,1\right]$ ist der sogenannte \textbf{Discount-Faktor}.
Für den Agenten werden nach jeder Aktion alle noch erreichbaren Belohnungen
mit $\gamma$ multipliziert, damit er kürzere Wege, die zur selben
Belohnung führen, bevorzugen wird. Mit $\gamma=1$ ist für den Agenten
ein langer Weg ebenso gut wie ein kurzer, solange er die selbe Belohnung
erhält. Mit $\gamma=0$ wird der Agent nicht mehr vorausschauend planen,
da für ihn nur die nächste Belohnung Wert hat. Es gilt also, einen
geeigneten Trade-Off zu finden.
\end{itemize}

\subsection{V-Values}

Um ein MDP zu lösen, lässt sich auf ein Bewertungsschema für Zustände
zurückgreifen. Hierbei ergibt sich die Bewertung eines Zustandes aus
der \textbf{Gesamtbelohung, die der Agent zu erwarteten hat, wenn
er eine Simulation in besagtem Zustand startet, die optimale Aktion
ausführt und von dort an optimal handelt}. Die Markov-Bedingung erlaubt
uns in jedem Zustand anzunehmen, wir hätten die Simulation gerade
erst gestartet, da es keine Faktoren gibt, die von bereits vergangenen
Ereignissen abhängen.

Da die Bewertung jedes Zustandes angibt, welche Belohnung von ihm
aus zu erwarten ist, lässt sich die Bewertung wie folgt rekursiv definieren:

\[
V^{\ast}\left(s\right)=\max_{a\in A}\sum_{s^{\prime}\in S}T\left(s,a,s^{\prime}\right)\left[R\left(s,a,s^{\prime}\right)+\gamma\cdot V^{\ast}\left(s^{\prime}\right)\right]
\]

In der eckigen Klammer befindet sich die unmittelbare Belohung, wenn
der Agent von Zustand $s$ über Aktion $a$ in Zustand $s^{\prime}$
gelandet ist, addiert zur zu erwartenden Belohnung von Zustand $s^{\prime}$
aus (welche mit $\gamma$ multipliziert wurde, da eine Aktion vorrüber
ist).

Dieser Wert wird mit der Wahrscheinlichkeit des Auftretens dieses
Übergangs multipliziert und durch Aufsummieren über alle möglichen
Zustände $s^{\prime}$ (\emph{man bedenke, dass $T$ nur für Zustände,
die von $s$ über $a$ erreicht werden können $>0$ ist}) zum Erwartungswert
für eine Aktion $a$.

Da $V^{\ast}$ als erwartete Belohnung definiert ist, wenn der Agent
immer optimal handelt, ist es der Erwartungswert der Aktion, von welcher
die höchste Belohnung erwartet wird.

\subsection{Value Iteration}

Durch die rekursive Definition von $V^{\ast}$ lässt es sich nicht
trivial berechnen. Als möglicher Lösungsansatz bietet sich daher die
Value Iteration an. Hierzu wird das MDP zeitlich begrenzt und $V^{\left(k\right)}$
definiert als \emph{Gesamtbelohung, die der Agent zu erwarteten hat,
wenn er eine Simulation in besagtem Zustand startet, die optimale
Aktion ausführt und von dort an optimal handelt, }\textbf{\emph{aber
nur noch $k$ Schritte übrig hat}}.

$V^{\left(0\right)}$ lässt sich einfach bestimmen, da es $0$ für
jeden Zustand sein muss. Auch $V^{\left(1\right)}$ folgt oft direkt
aus der Problemdefinition. In unserem Beispiel wäre es $-100$ bzw
$+100$ für die ``bunten'' Zustände und $-2$ für alle anderen,
da jeder Schritt $-2$ Belohnung bringt.

Ein beliebiges $V^{\left(k\right)}$ lässt sich einfach berechnen.
Dazu muss nur die Formel für $V^{\ast}$leicht angepasst werden:

\[
V^{\left(k\right)}\left(s\right)=\max_{a\in A}\sum_{s^{\prime}\in S}T\left(s,a,s^{\prime}\right)\left[R\left(s,a,s^{\prime}\right)+\gamma\cdot V^{\left(k-1\right)}\left(s^{\prime}\right)\right]
\]

Da sich das hintere $V^{\ast}$ in der Gleichung ja auf Geschehnisse
bezog, die einen Zeitschritt später passieren, muss es bei der Berechnung
von $V^{\left(k\right)}$ durch ein $V^{\left(k-1\right)}$ ersetzt
werden, da der Agent im zeitlich beschränkten MDP einen Zeitschritt
später selbstverständlich nur noch einen Zeitschritt weniger zur Verfügung
hat. 

\includegraphics[width=0.95\textwidth]{media/example_value_iteration}

Für MDPs, deren maximal erzielbare Belohung nach oben beschränkt ist,
wird $V^{\left(k\right)}$ für $k\longrightarrow\infty$ konvergieren,
da sich auch mit mehr Schritten kein besseres Ergebnis mehr erzielen
lassen wird. Durch ein $\gamma<1$ kann dies <> werden.

Für ein geeignet großes $k$ gilt also $V^{\left(k\right)}=V^{\ast}\pm\varepsilon$
mit vernachlässigbarem $\varepsilon$ (\emph{z.B. kleiner als Maschinengenauigkeit
oder kleiner als eine vorher gewählte Toleranz}), da $V^{\left(\infty\right)}$
genau unserer Definition für ein $V^{\ast}$ im nicht zeitlich beschränkten
MDP entspricht.

\subsection{Policy Extraction}

Mit Hilfe der optimalen V-Values $V^{\ast}$ lässt sich jetzt eine
optimale Policy für das MDP berechnen. Dazu simuliert man einen weiteren
Schritt der Value Iteration. Anstatt nun aber tatsächlich das Maximum
über alle $a$ zu bestimmen, weisen wir dem aktullen Zustand die Aktion
zu, die von ihm aus die erwartete Belohnung maximieren würde.

\[
\pi^{*}\left(s\right)=\underset{a\in A}{\textrm{argmax}}\sum_{s^{\prime}\in S}T\left(s,a,s^{\prime}\right)\left[R\left(s,a,s^{\prime}\right)+\gamma\cdot V^{\ast}\left(s^{\prime}\right)\right]
\]

Auf diese Weise haben wir eine optimale Strategie erzeugt und somit
das MDP gelöst.

\includegraphics[width=0.62\textwidth]{media/example_policy_extraction}

\section{Reinforcement Learning}

Reinforcement Learning ist ein Machine-Learning-Konzept, in welchem
ein Agent ohne anfängliche Informationen über sein Umfeld oder die
Auswirkungen seines Handelns dazu trainiert werden soll, die maximale
Belohnung zu erhalten. 

\subsection{Verbindung zu MDPs}

Während sich zum Beispiel mit Value Iteration und Policy Extraction
MDPs lösen lassen, bei denen alle Informationen vorhanden sind, bietet
Reinforcement Learning eine Reihe von Algorithmen bei welchen vom
Agenten keine Kenntnis von $T\left(s,a,s^{\prime}\right)$ oder $R\left(s,a,s^{\prime}\right)$
vorrausgesetzt werden. Der Agent muss sich also selbstständig über
Aktionen durch die Zustände bewegen, um die verschiedenen Übergänge
und deren Belohnungen und Wahrscheinlichkeiten selbst zu erfahren.

Da der Agent erst nach jedem Übergang die erhaltene Belohnung von
extern mitgeteilt bekommt, lassen sich so bei der Problemmodellierung
einfache Möglichkeiten implementieren, Belohnungen zu verteilen. 

\subsection{Model Based Approaches}

Die erste Möglichkeit an dieses Problem heranzugehen wäre ein \textbf{model
based approach}. Das bedeutet, der Agent versucht zunächst, die fehlenden
Teile des Modells (also vor allem $T$ und $R$) durch Samples zu
approximieren. Sobald diese annährend bekannt sind, lässt sich das
Problem mit beliebigen MDP-Algorithmen (wie zum Beispiel Value Iteration
und Policy Extraction) lösen. 

Dieser Ansatz erfordert sehr große Mengen an Samples welche zur Auswertung
gleichzeitig zur Verfügung stehen müssen. Ausserdem erfordert das
Sammeln der Samples einen großen Grad an Freiheit in der Bewegung
des Agenten durch die Zustände, was zum Beispiel bei Anwendungen in
der Robotik oft nicht gegeben ist. Bei sehr großen (oder sogar indiskreten)
Zustands-Mengen ist dieser Ansatz also nur schwer umsetzbar.

\subsection{Temporal Difference Learning}

Als weiterer Ansatz bietet sich das \textbf{temporal difference learning}
an. Die Idee dahinter ist, dass mit zufällig gewählten V-Values begonnen
wird und diese langsam iterativ verbessert werden.

Nach jeder Aktion (und deren Feedback) überprüft der Agent wie sich
die ihm über V-Values versprochene erwartete Belohunung im Vergleich
zur direkt erhaltenen Belohung und der nun in Aussicht stehenden erwarteten
Belohnung verhält und passt die V-Values des eben verlassenen Zustandes
geringfügig dementsprechend an.

\[
V\left(s\right)\leftarrow V\left(s\right)+\alpha\left[r+\gamma V\left(s^{\prime}\right)-V\left(s\right)\right]
\]
 Die Bewertung von Zustand $s$ wird um einen kleinen Teil $\left(\alpha<0\right)$
der Differenz zwischen versprochener $\left(V\left(s\right)\right)$
und erfahrener $\left(r+\gamma V\left(s^{\prime}\right)\right)$ Belohung
angepasst. Die Lernrate (\emph{learning rate}) $\alpha$ ist dabei
ein wichtiger Hyperparameter und entscheident für die Konvergenz des
Verfahrens. Für ein sich verringerndes $\alpha_{i}$ ($\alpha$ im
$i$-ten Zeitschritt) mit $\sum a_{i}=\infty$ und $\sum a_{i}^{2}<\infty$
konvergiert das Verfahren in jedem Fall gegen die optimale Lösung
(sofern es eine gibt). Aber auch für konstante, sehr kleine $\alpha$
konvergiert das Verfahren in der Praxis sehr oft.

\subsection{Q-Values/Q-Learning}

\subsection{Exploration}

\subsection{State Features}

\subsection{Andere Algoritmen}

\subsection{Deep Q-Learning}

\section{Fazit}

\section{Kurzgeschichte (?)}

\section{Quellen}
\end{document}
