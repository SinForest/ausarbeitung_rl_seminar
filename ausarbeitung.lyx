#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 3cm
\rightmargin 2.5cm
\bottommargin 3.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Abstract
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
ToDo
\end_layout

\end_inset


\end_layout

\begin_layout Section
Markov Entscheidungs-Probleme
\end_layout

\begin_layout Standard
Markov Entscheidungs-Probleme (kurz 
\series bold
MDP
\series default
 für 
\emph on
markov decision process
\emph default
) sind ein Modell für Probleme, bei denen ein Agent versucht die größtmögliche
 Belohnung (
\emph on
reward
\emph default
) zu erzielen.
 Er bewegt sich dazu durch eine Menge von Zuständen (
\emph on
states
\emph default
) indem er aus einer Menge von Aktionen (
\emph on
actions
\emph default
) wählt.
 Welchen Zustand der Agent erreicht ist nicht deterministisch, die Wahrscheinlic
hkeiten sind jedoch nur von der gewählten Aktion und dem aktuellen Zustand
 abhängig.
 Die Belohnung, die der Agent für einen Übergang erhält, wird durch Ausgangs-
 und Endzustand des Übergangs so wie die gewählte Aktion bestimmt.
 Der Agent startet in einem Startzustand, es kann diverse Endzustände geben,
 nach deren Erreichen keine Aktionen mehr ausgeführt werden können.
\end_layout

\begin_layout Standard
Ziel ist es, eine Strategie (
\emph on
policy
\emph default
) zu finden, welche jedem Zustand die Aktion zuordnet, durch die der höchste
 Gesamtgewinn (also die höchste Summe über alle erzielten Gewinne) erwartet
 werden kann.
\end_layout

\begin_layout Subsection
Ein Beispiel
\end_layout

\begin_layout Standard
In diesem Beispiel werden die Zustände durch die Felder eines 
\begin_inset Formula $4\times4$
\end_inset

-Grids visualisiert.
 Der Startzustand 
\begin_inset Formula $\left(3,0\right)$
\end_inset

 ist durch ein großes 
\begin_inset Formula $S$
\end_inset

 markiert, die Zustände mit einem 
\begin_inset Formula $X$
\end_inset

 sind nicht erreichbar.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename media/example.svg
	lyxscale 15
	scale 35

\end_inset


\end_layout

\begin_layout Standard
Die Menge der Aktionen besteht aus 4 Elementen, jedes symbolisiert einen
 Schritt in eine der vier Himmelsrichtungen.
 Beim Ausführen einer Aktion landet der Agent mit einer Chance von 
\begin_inset Formula $0.8$
\end_inset

 ein Feld weiter in der gewählten Himmelsrichtung, so wie mit je einer Chance
 von 
\begin_inset Formula $0.1$
\end_inset

 in einer der zwei orthogonalen Richtungen.
 Würde der Agent hierbei aif einem Feld landen, welches er nicht betreten
 kann (eines der mit 
\begin_inset Formula $X$
\end_inset

 markierten, oder außerhalb des Grids), bleibt er auf seinem Feld.
\end_layout

\begin_layout Standard
Von den bunten Feldern aus führt jede Aktion mit einer Chance von 
\begin_inset Formula $1$
\end_inset

 in den Endzustand.
 Der Agent erhält für diesen Übergang die auf dem Feld verzeichnete Belohnung
 (100 oder -100).
 Für jede andere Aktion wird eine Belohnung von -2 verbucht.
\end_layout

\begin_layout Subsection
Notation
\end_layout

\begin_layout Standard
Zur Beschreibung eines MDPs werden einige Notationen benötigt, die hier
 angegeben werden.
 
\emph on
(Zu einigen Notationen werden Beispiele zum oben genannten Beispiel angegeben.)
\emph default
 Zur Problemstellung selbst gehören:
\end_layout

\begin_layout Itemize
\begin_inset Formula $S=\left\{ s_{1},\ldots,s_{n}\right\} $
\end_inset

 ist die Menge der 
\series bold
Zustände
\series default
.
 Die Elemente der Menge können beliebig benannt werden.
\end_layout

\begin_layout Itemize
\begin_inset Formula $A=\left\{ a_{1},\ldots,a_{n}\right\} $
\end_inset

 ist die Menge der 
\series bold
Aktionen
\series default
.
 Auch diese Elemente können beliebig benannt werden.
\end_layout

\begin_layout Itemize
\begin_inset Formula $T:S\times A\times S\rightarrow\left[0,1\right]$
\end_inset

 ist die 
\series bold
Übergangs-Funktion
\series default
 (
\emph on
transition function
\emph default
).
 
\begin_inset Formula $T\left(s,a,s^{\prime}\right)$
\end_inset

 beschreibt die Wahrscheinlichkeit, mit der der Agent von Zustand 
\begin_inset Formula $s$
\end_inset

 in den Zustand 
\begin_inset Formula $s^{\prime}$
\end_inset

 wechselt, wenn er Aktion 
\begin_inset Formula $a$
\end_inset

 ausführt.
 Eine allgemeinere, jedoch hier nicht verwendete Schreibweise wäre 
\begin_inset Formula $T\left(s,a,s^{\prime}\right)=p\left(s^{\prime}|s,a\right)$
\end_inset

.
\begin_inset Formula 
\begin{align*}
T(s_{0,2},a_{E},s_{0,3}) & =0.8 & \text{(Agent wählt Osten und kommt dort an)}\\
T(s_{0,2},a_{E},s_{0,2}) & =0.1 & \text{(Agent wählt Osten, geht nach Norden, kein valider Zug)}\\
T(s_{0,2},a_{E},s_{1,2}) & =0.1 & \text{(Agent wählt Osten, geht nach Süden)}\\
T(s_{0,2},a_{E},s_{0,1}) & =0.0 & \text{(Agent wählt Osten, Bewegung nach Westen unmöglich)}\\
T(s_{1,2},\_,s_{end}) & =1.0 & \text{(jede Aktion von bunten Feldern führt zum Endzustand)}
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $R:S\times A\times S\rightarrow\mathbb{R}$
\end_inset

 ist die 
\series bold
Belohnungs-Funktion
\series default
 (
\emph on
reward function
\emph default
).
 
\begin_inset Formula $R\left(s,a,s^{\prime}\right)$
\end_inset

 ordnet dem Übergang von Zustand 
\begin_inset Formula $s$
\end_inset

 nach Zustand 
\begin_inset Formula $s^{\prime}$
\end_inset

 mit Hilfe von Aktion 
\begin_inset Formula $a$
\end_inset

 eine Belohung zu.
\begin_inset Formula 
\begin{align*}
R(s_{1,2},\_,s_{end}) & =-100 & \text{(Belohnung von buntem Feld)}\\
R(s_{1,3},\_,s_{end}) & =+100 & \text{(Belohnung von buntem Feld)}\\
R(s_{0,2},a_{N},s_{0,1}) & =-2 & \text{(jede Bewegung kostet 2 Belohung)}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Zum Lösen der Probleme werden folgende Notationen verwendet:
\end_layout

\begin_layout Itemize
\begin_inset Formula $\pi\left(s\right)$
\end_inset

 ist eine 
\series bold
Policy
\series default
, die dem Zustand 
\begin_inset Formula $s$
\end_inset

 die optimale Aktion zuordnet.
 Hierbei beschreibt 
\begin_inset Formula $\pi$
\end_inset

 eine Policy im Allgemeinen, 
\begin_inset Formula $\pi^{\ast}$
\end_inset

 die optimale Policy.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\gamma\in\left[0,1\right]$
\end_inset

 ist der sogenannte 
\series bold
Discount-Faktor
\series default
.
 Für den Agenten werden nach jeder Aktion alle noch erreichbaren Belohnungen
 mit 
\begin_inset Formula $\gamma$
\end_inset

 multipliziert, damit er kürzere Wege, die zur selben Belohnung führen,
 bevorzugen wird.
 Mit 
\begin_inset Formula $\gamma=1$
\end_inset

 ist für den Agenten ein langer Weg ebenso gut wie ein kurzer, solange er
 die selbe Belohnung erhält.
 Mit 
\begin_inset Formula $\gamma=0$
\end_inset

 wird der Agent nicht mehr vorausschauend planen, da für ihn nur die nächste
 Belohnung Wert hat.
 Es gilt also, einen geeigneten Trade-Off zu finden.
\end_layout

\begin_layout Subsection
V-Values
\end_layout

\begin_layout Standard
Um ein MDP zu lösen, lässt sich auf ein Bewertungsschema für Zustände zurückgrei
fen.
 Hierbei ergibt sich die Bewertung eines Zustandes aus der 
\series bold
Gesamtbelohung, die der Agent zu erwarteten hat, wenn er eine Simulation
 in besagtem Zustand startet, die optimale Aktion ausführt und von dort
 an optimal handelt
\series default
.
 Die Markov-Bedingung erlaubt uns in jedem Zustand anzunehmen, wir hätten
 die Simulation gerade erst gestartet, da es keine Faktoren gibt, die von
 bereits vergangenen Ereignissen abhängen.
\end_layout

\begin_layout Standard
Da die Bewertung jedes Zustandes angibt, welche Belohnung von ihm aus zu
 erwarten ist, lässt sich die Bewertung wie folgt rekursiv definieren:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
V^{\ast}\left(s\right)=\max_{a\in A}\sum_{s^{\prime}\in S}T\left(s,a,s^{\prime}\right)\left[R\left(s,a,s^{\prime}\right)+\gamma\cdot V^{\ast}\left(s^{\prime}\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
In der eckigen Klammer befindet sich die unmittelbare Belohung, wenn der
 Agent von Zustand 
\begin_inset Formula $s$
\end_inset

 über Aktion 
\begin_inset Formula $a$
\end_inset

 in Zustand 
\begin_inset Formula $s^{\prime}$
\end_inset

 gelandet ist, addiert zur zu erwartenden Belohnung von Zustand 
\begin_inset Formula $s^{\prime}$
\end_inset

 aus (welche mit 
\begin_inset Formula $\gamma$
\end_inset

 multipliziert wurde, da eine Aktion vorrüber ist).
\end_layout

\begin_layout Standard
Dieser Wert wird mit der Wahrscheinlichkeit des Auftretens dieses Übergangs
 multipliziert und durch Aufsummieren über alle möglichen Zustände 
\begin_inset Formula $s^{\prime}$
\end_inset

 (
\emph on
man bedenke, dass 
\begin_inset Formula $T$
\end_inset

 nur für Zustände, die von 
\begin_inset Formula $s$
\end_inset

 über 
\begin_inset Formula $a$
\end_inset

 erreicht werden können 
\begin_inset Formula $>0$
\end_inset

 ist
\emph default
) zum Erwartungswert für eine Aktion 
\begin_inset Formula $a$
\end_inset

.
\end_layout

\begin_layout Standard
Da 
\begin_inset Formula $V^{\ast}$
\end_inset

 als erwartete Belohnung definiert ist, wenn der Agent immer optimal handelt,
 ist es der Erwartungswert der Aktion, von welcher die höchste Belohnung
 erwartet wird.
\end_layout

\begin_layout Subsection
Value Iteration
\end_layout

\begin_layout Standard
Durch die rekursive Definition von 
\begin_inset Formula $V^{\ast}$
\end_inset

 lässt es sich nicht trivial berechnen.
 Als möglicher Lösungsansatz bietet sich daher die Value Iteration an.
 Hierzu wird das MDP zeitlich begrenzt und 
\begin_inset Formula $V^{\left(k\right)}$
\end_inset

 definiert als 
\emph on
Gesamtbelohung, die der Agent zu erwarteten hat, wenn er eine Simulation
 in besagtem Zustand startet, die optimale Aktion ausführt und von dort
 an optimal handelt, 
\series bold
aber nur noch 
\begin_inset Formula $k$
\end_inset

 Schritte übrig hat
\series default
\emph default
.
\end_layout

\begin_layout Standard
\begin_inset Formula $V^{\left(0\right)}$
\end_inset

 lässt sich einfach bestimmen, da es 
\begin_inset Formula $0$
\end_inset

 für jeden Zustand sein muss.
 Auch 
\begin_inset Formula $V^{\left(1\right)}$
\end_inset

 folgt oft direkt aus der Problemdefinition.
 In unserem Beispiel wäre es 
\begin_inset Formula $-100$
\end_inset

 bzw 
\begin_inset Formula $+100$
\end_inset

 für die 
\begin_inset Quotes eld
\end_inset

bunten
\begin_inset Quotes erd
\end_inset

 Zustände und 
\begin_inset Formula $-2$
\end_inset

 für alle anderen, da jeder Schritt 
\begin_inset Formula $-2$
\end_inset

 Belohnung bringt.
\end_layout

\begin_layout Standard
Ein beliebiges 
\begin_inset Formula $V^{\left(k\right)}$
\end_inset

 lässt sich einfach berechnen.
 Dazu muss nur die Formel für 
\begin_inset Formula $V^{\ast}$
\end_inset

leicht angepasst werden:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
V^{\left(k\right)}\left(s\right)=\max_{a\in A}\sum_{s^{\prime}\in S}T\left(s,a,s^{\prime}\right)\left[R\left(s,a,s^{\prime}\right)+\gamma\cdot V^{\left(k-1\right)}\left(s^{\prime}\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Da sich das hintere 
\begin_inset Formula $V^{\ast}$
\end_inset

 in der Gleichung ja auf Geschehnisse bezog, die einen Zeitschritt später
 passieren, muss es bei der Berechnung von 
\begin_inset Formula $V^{\left(k\right)}$
\end_inset

 durch ein 
\begin_inset Formula $V^{\left(k-1\right)}$
\end_inset

 ersetzt werden, da der Agent im zeitlich beschränkten MDP einen Zeitschritt
 später selbstverständlich nur noch einen Zeitschritt weniger zur Verfügung
 hat.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
kürzbar, wenn zu viel
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename media/example_value_iteration.svg
	lyxscale 20
	width 95text%

\end_inset


\end_layout

\begin_layout Standard
Für MDPs, deren maximal erzielbare Belohung nach oben beschränkt ist, wird
 
\begin_inset Formula $V^{\left(k\right)}$
\end_inset

 für 
\begin_inset Formula $k\longrightarrow\infty$
\end_inset

 konvergieren, da sich auch mit mehr Schritten kein besseres Ergebnis mehr
 erzielen lassen wird.
 Durch ein 
\begin_inset Formula $\gamma<1$
\end_inset

 kann dies <
\begin_inset Note Note
status open

\begin_layout Plain Layout
VERB
\end_layout

\end_inset

> werden.
\end_layout

\begin_layout Standard
Für ein geeignet großes 
\begin_inset Formula $k$
\end_inset

 gilt also 
\begin_inset Formula $V^{\left(k\right)}=V^{\ast}\pm\varepsilon$
\end_inset

 mit vernachlässigbarem 
\begin_inset Formula $\varepsilon$
\end_inset

 (
\emph on
z.B.
 kleiner als Maschinengenauigkeit oder kleiner als eine vorher gewählte
 Toleranz
\emph default
), da 
\begin_inset Formula $V^{\left(\infty\right)}$
\end_inset

 genau unserer Definition für ein 
\begin_inset Formula $V^{\ast}$
\end_inset

 im nicht zeitlich beschränkten MDP entspricht.
\end_layout

\begin_layout Subsection
Policy Extraction
\end_layout

\begin_layout Standard
Mit Hilfe der optimalen V-Values 
\begin_inset Formula $V^{\ast}$
\end_inset

 lässt sich jetzt eine optimale Policy für das MDP berechnen.
 Dazu simuliert man einen weiteren Schritt der Value Iteration.
 Anstatt nun aber tatsächlich das Maximum über alle 
\begin_inset Formula $a$
\end_inset

 zu bestimmen, weisen wir dem aktullen Zustand die Aktion zu, die von ihm
 aus die erwartete Belohnung maximieren würde.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\pi^{*}\left(s\right)=\underset{a\in A}{\textrm{argmax}}\sum_{s^{\prime}\in S}T\left(s,a,s^{\prime}\right)\left[R\left(s,a,s^{\prime}\right)+\gamma\cdot V^{\ast}\left(s^{\prime}\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Auf diese Weise haben wir eine optimale Strategie erzeugt und somit das
 MDP gelöst.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename media/example_policy_extraction.svg
	lyxscale 20
	width 62text%

\end_inset


\end_layout

\begin_layout Section
Reinforcement Learning
\end_layout

\begin_layout Standard
Reinforcement Learning ist ein Machine-Learning-Konzept, in welchem ein
 Agent ohne anfängliche Informationen über sein Umfeld oder die Auswirkungen
 seines Handelns dazu trainiert werden soll, die maximale Belohnung zu erhalten.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
mehr? mit 3.1 zusammenlegen?
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Verbindung zu MDPs
\end_layout

\begin_layout Standard
Während sich zum Beispiel mit Value Iteration und Policy Extraction MDPs
 lösen lassen, bei denen alle Informationen vorhanden sind, bietet Reinforcement
 Learning eine Reihe von Algorithmen bei welchen vom Agenten keine Kenntnis
 von 
\begin_inset Formula $T\left(s,a,s^{\prime}\right)$
\end_inset

 oder 
\begin_inset Formula $R\left(s,a,s^{\prime}\right)$
\end_inset

 vorrausgesetzt werden.
 Der Agent muss sich also selbstständig über Aktionen durch die Zustände
 bewegen, um die verschiedenen Übergänge und deren Belohnungen und Wahrscheinlic
hkeiten selbst zu erfahren.
\end_layout

\begin_layout Standard
Da der Agent erst nach jedem Übergang die erhaltene Belohnung von extern
 mitgeteilt bekommt, lassen sich so bei der Problemmodellierung einfache
 Möglichkeiten implementieren, Belohnungen zu verteilen.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Beispiel?
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Model Based Approaches
\end_layout

\begin_layout Standard
Die erste Möglichkeit an dieses Problem heranzugehen wäre ein 
\series bold
model based approach
\series default
.
 Das bedeutet, der Agent versucht zunächst, die fehlenden Teile des Modells
 (also vor allem 
\begin_inset Formula $T$
\end_inset

 und 
\begin_inset Formula $R$
\end_inset

) durch Samples zu approximieren.
 Sobald diese annährend bekannt sind, lässt sich das Problem mit beliebigen
 MDP-Algorithmen (wie zum Beispiel Value Iteration und Policy Extraction)
 lösen.
 
\end_layout

\begin_layout Standard
Dieser Ansatz erfordert sehr große Mengen an Samples welche zur Auswertung
 gleichzeitig zur Verfügung stehen müssen.
 Ausserdem erfordert das Sammeln der Samples einen großen Grad an Freiheit
 in der Bewegung des Agenten durch die Zustände, was zum Beispiel bei Anwendunge
n in der Robotik oft nicht gegeben ist.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Beispiel? Fußnote?
\end_layout

\end_inset

 Bei sehr großen (oder sogar indiskreten) Zustands-Mengen ist dieser Ansatz
 also nur schwer umsetzbar.
\end_layout

\begin_layout Subsection
Temporal Difference Learning
\end_layout

\begin_layout Standard
Als weiterer Ansatz bietet sich das 
\series bold
temporal difference learning
\series default
 an.
 Die Idee dahinter ist, dass mit zufällig gewählten V-Values begonnen wird
 und diese langsam iterativ verbessert werden.
\end_layout

\begin_layout Standard
Nach jeder Aktion (und deren Feedback) überprüft der Agent wie sich die
 ihm über V-Values versprochene erwartete Belohunung im Vergleich zur direkt
 erhaltenen Belohung und der nun in Aussicht stehenden erwarteten Belohnung
 verhält und passt die V-Values des eben verlassenen Zustandes geringfügig
 dementsprechend an.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
V\left(s\right)\leftarrow V\left(s\right)+\alpha\left[r+\gamma V\left(s^{\prime}\right)-V\left(s\right)\right]
\]

\end_inset

 Die Bewertung von Zustand 
\begin_inset Formula $s$
\end_inset

 wird um einen kleinen Teil 
\begin_inset Formula $\left(\alpha<0\right)$
\end_inset

 der Differenz zwischen versprochener 
\begin_inset Formula $\left(V\left(s\right)\right)$
\end_inset

 und erfahrener 
\begin_inset Formula $\left(r+\gamma V\left(s^{\prime}\right)\right)$
\end_inset

 Belohung angepasst.
 Die Lernrate (
\emph on
learning rate
\emph default
) 
\begin_inset Formula $\alpha$
\end_inset

 ist dabei ein wichtiger Hyperparameter und entscheident für die Konvergenz
 des Verfahrens.
 Für ein sich verringerndes 
\begin_inset Formula $\alpha_{i}$
\end_inset

 (
\begin_inset Formula $\alpha$
\end_inset

 im 
\begin_inset Formula $i$
\end_inset

-ten Zeitschritt) mit 
\begin_inset Formula $\sum a_{i}=\infty$
\end_inset

 und 
\begin_inset Formula $\sum a_{i}^{2}<\infty$
\end_inset

 konvergiert das Verfahren in jedem Fall gegen die optimale Lösung (sofern
 es eine gibt).
 Aber auch für konstante, sehr kleine 
\begin_inset Formula $\alpha$
\end_inset

 konvergiert das Verfahren in der Praxis sehr oft.
\end_layout

\begin_layout Subsection
Q-Values/Q-Learning
\end_layout

\begin_layout Subsection
Exploration
\end_layout

\begin_layout Subsection
State Features
\end_layout

\begin_layout Subsection
Andere Algoritmen
\end_layout

\begin_layout Subsection
Deep Q-Learning
\end_layout

\begin_layout Section
Fazit
\end_layout

\begin_layout Section
Kurzgeschichte (?)
\end_layout

\begin_layout Section
Quellen
\end_layout

\end_body
\end_document
