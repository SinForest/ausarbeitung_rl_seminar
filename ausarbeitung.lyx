#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 3cm
\rightmargin 2.5cm
\bottommargin 3.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Abstract
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
ToDo
\end_layout

\end_inset


\end_layout

\begin_layout Section
Markov Entscheidungs-Probleme
\end_layout

\begin_layout Standard
Markov Entscheidungs-Probleme (kurz 
\series bold
MDP
\series default
 für 
\emph on
markov decision process
\emph default
) sind ein Modell für Probleme, bei denen ein Agent versucht die größtmögliche
 Belohnung (
\emph on
reward
\emph default
) zu erzielen.
 Er bewegt sich dazu durch eine Menge von Zuständen (
\emph on
states
\emph default
) indem er aus einer Menge von Aktionen (
\emph on
actions
\emph default
) wählt.
 Welchen Zustand der Agent erreicht ist nicht deterministisch, die Wahrscheinlic
hkeiten sind jedoch nur von der gewählten Aktion und dem aktuellen Zustand
 abhängig.
 Die Belohnung, die der Agent für einen Übergang erhält, wird durch Ausgangs-
 und Endzustand des Übergangs so wie die gewählte Aktion bestimmt.
 Der Agent startet in einem Startzustand, es kann diverse Endzustände geben,
 nach deren Erreichen keine Aktionen mehr ausgeführt werden können.
\end_layout

\begin_layout Standard
Ziel ist es, eine Strategie (
\emph on
policy
\emph default
) zu finden, welche jedem Zustand die Aktion zuordnet, durch die der höchste
 Gesamtgewinn (also die höchste Summe über alle erzielten Gewinne) erwartet
 werden kann.
\end_layout

\begin_layout Subsection
Ein Beispiel
\end_layout

\begin_layout Standard
In diesem Beispiel werden die Zustände durch die Felder eines 
\begin_inset Formula $4\times4$
\end_inset

-Grids visualisiert.
 Der Startzustand 
\begin_inset Formula $\left(3,0\right)$
\end_inset

 ist durch ein großes 
\begin_inset Formula $S$
\end_inset

 markiert, die Zustände mit einem 
\begin_inset Formula $X$
\end_inset

 sind nicht erreichbar.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename media/example.svg
	lyxscale 15
	scale 35

\end_inset


\end_layout

\begin_layout Standard
Die Menge der Aktionen besteht aus 4 Elementen, jedes symbolisiert einen
 Schritt in eine der vier Himmelsrichtungen.
 Beim Ausführen einer Aktion landet der Agent mit einer Chance von 
\begin_inset Formula $0.8$
\end_inset

 ein Feld weiter in der gewählten Himmelsrichtung, so wie mit je einer Chance
 von 
\begin_inset Formula $0.1$
\end_inset

 in einer der zwei orthogonalen Richtungen.
 Würde der Agent hierbei aif einem Feld landen, welches er nicht betreten
 kann (eines der mit 
\begin_inset Formula $X$
\end_inset

 markierten, oder außerhalb des Grids), bleibt er auf seinem Feld.
\end_layout

\begin_layout Standard
Von den bunten Feldern aus führt jede Aktion mit einer Chance von 
\begin_inset Formula $1$
\end_inset

 in den Endzustand.
 Der Agent erhält für diesen Übergang die auf dem Feld verzeichnete Belohnung
 (100 oder -100).
 Für jede andere Aktion wird eine Belohnung von -2 verbucht.
\end_layout

\begin_layout Subsection
Notation
\end_layout

\begin_layout Standard
Zur Beschreibung eines MDPs werden einige Notationen benötigt, die hier
 angegeben werden.
 
\emph on
(Zu einigen Notationen werden Beispiele zum oben genannten Beispiel angegeben.)
\emph default
 Zur Problemstellung selbst gehören:
\end_layout

\begin_layout Itemize
\begin_inset Formula $S=\left\{ s_{1},\ldots,s_{n}\right\} $
\end_inset

 ist die Menge der 
\series bold
Zustände
\series default
.
 Die Elemente der Menge können beliebig benannt werden.
\end_layout

\begin_layout Itemize
\begin_inset Formula $A=\left\{ a_{1},\ldots,a_{n}\right\} $
\end_inset

 ist die Menge der 
\series bold
Aktionen
\series default
.
 Auch diese Elemente können beliebig benannt werden.
\end_layout

\begin_layout Itemize
\begin_inset Formula $T:S\times A\times S\rightarrow\left[0,1\right]$
\end_inset

 ist die 
\series bold
Übergangs-Funktion
\series default
 (
\emph on
transition function
\emph default
).
 
\begin_inset Formula $T\left(s,a,s^{\prime}\right)$
\end_inset

 beschreibt die Wahrscheinlichkeit, mit der der Agent von Zustand 
\begin_inset Formula $s$
\end_inset

 in den Zustand 
\begin_inset Formula $s^{\prime}$
\end_inset

 wechselt, wenn er Aktion 
\begin_inset Formula $a$
\end_inset

 ausführt.
 Eine allgemeinere, jedoch hier nicht verwendete Schreibweise wäre 
\begin_inset Formula $T\left(s,a,s^{\prime}\right)=p\left(s^{\prime}|s,a\right)$
\end_inset

.
\begin_inset Formula 
\begin{align*}
T(s_{0,2},a_{E},s_{0,3}) & =0.8 & \text{(Agent wählt Osten und kommt dort an)}\\
T(s_{0,2},a_{E},s_{0,2}) & =0.1 & \text{(Agent wählt Osten, geht nach Norden, kein valider Zug)}\\
T(s_{0,2},a_{E},s_{1,2}) & =0.1 & \text{(Agent wählt Osten, geht nach Süden)}\\
T(s_{0,2},a_{E},s_{0,1}) & =0.0 & \text{(Agent wählt Osten, Bewegung nach Westen unmöglich)}\\
T(s_{1,2},\_,s_{end}) & =1.0 & \text{(jede Aktion von bunten Feldern führt zum Endzustand)}
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $R:S\times A\times S\rightarrow\mathbb{R}$
\end_inset

 ist die 
\series bold
Belohnungs-Funktion
\series default
 (
\emph on
reward function
\emph default
).
 
\begin_inset Formula $R\left(s,a,s^{\prime}\right)$
\end_inset

 ordnet dem Übergang von Zustand 
\begin_inset Formula $s$
\end_inset

 nach Zustand 
\begin_inset Formula $s^{\prime}$
\end_inset

 mit Hilfe von Aktion 
\begin_inset Formula $a$
\end_inset

 eine Belohung zu.
\begin_inset Formula 
\begin{align*}
R(s_{1,2},\_,s_{end}) & =-100 & \text{(Belohnung von buntem Feld)}\\
R(s_{1,3},\_,s_{end}) & =+100 & \text{(Belohnung von buntem Feld)}\\
R(s_{0,2},a_{N},s_{0,1}) & =-2 & \text{(jede Bewegung kostet 2 Belohung)}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Zum Lösen der Probleme werden folgende Notationen verwendet:
\end_layout

\begin_layout Itemize
\begin_inset Formula $\pi\left(s\right)$
\end_inset

 ist eine 
\series bold
Policy
\series default
, die dem Zustand 
\begin_inset Formula $s$
\end_inset

 die optimale Aktion zuordnet.
 Hierbei beschreibt 
\begin_inset Formula $\pi$
\end_inset

 eine Policy im Allgemeinen, 
\begin_inset Formula $\pi^{\ast}$
\end_inset

 die optimale Policy.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\gamma\in\left[0,1\right]$
\end_inset

 ist der sogenannte 
\series bold
Discount-Faktor
\series default
.
 Für den Agenten werden nach jeder Aktion alle noch erreichbaren Belohnungen
 mit 
\begin_inset Formula $\gamma$
\end_inset

 multipliziert, damit er kürzere Wege, die zur selben Belohnung führen,
 bevorzugen wird.
 Mit 
\begin_inset Formula $\gamma=1$
\end_inset

 ist für den Agenten ein langer Weg ebenso gut wie ein kurzer, solange er
 die selbe Belohnung erhält.
 Mit 
\begin_inset Formula $\gamma=0$
\end_inset

 wird der Agent nicht mehr vorausschauend planen, da für ihn nur die nächste
 Belohnung Wert hat.
 Es gilt also, einen geeigneten Trade-Off zu finden.
\end_layout

\begin_layout Subsection
V-Values
\end_layout

\begin_layout Standard
Um ein MDP zu lösen, lässt sich auf ein Bewertungsschema für Zustände zurückgrei
fen.
 Hierbei ergibt sich die Bewertung eines Zustandes aus der Gesamtbelohung,
 die der Agent zu erwarteten hat, wenn er eine Simulation in besagtem Zustand
 startet und von dort aus optimal handelt.
 Die Markov-Bedingung erlaubt uns in jedem Zustand anzunehmen, wir hätten
 die Simulation gerade erst gestartet, da es keine Faktoren gibt, die von
 bereits vergangenen Ereignissen abhängen.
\end_layout

\begin_layout Standard
Da die Bewertung jedes Zustandes angibt, welche Belohnung von ihm aus zu
 erwarten ist, lässt sich die Bewertung wie folgt rekursiv definieren:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
V^{\ast}\left(s\right)=\max_{a\in A}\sum_{s^{\prime}\in S}T\left(s,a,s^{\prime}\right)\left[R\left(s,a,s^{\prime}\right)+\gamma\cdot V^{\ast}\left(s^{\prime}\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
In der eckigen Klammer befindet sich die unmittelbare Belohung, wenn ich
 von Zustand 
\begin_inset Formula $s$
\end_inset

 über Aktion 
\begin_inset Formula $a$
\end_inset

 in Zustand 
\begin_inset Formula $s^{\prime}$
\end_inset

 gelandet bin, addiert zur zu erwartenden Belohnung von Zustand 
\begin_inset Formula $s^{\prime}$
\end_inset

 aus (welche mit 
\begin_inset Formula $\gamma$
\end_inset

 multipliziert wurde, da eine Aktion vorrüber ist).
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Dieser Wert wird mit der Wahrscheinlichkeit des Auftretens dieses Übergangs
 multipliziert.
 Da diese nun für alle von 
\begin_inset Formula $s$
\end_inset

 über 
\begin_inset Formula $a$
\end_inset

 erreichbaren Zustände 
\begin_inset Formula $s^{\prime}$
\end_inset

 berechnet und aufsummiert werden, 
\end_layout

\begin_layout Plain Layout
Den so entstandenen Erwatungswert berechnet man für alle Aktionen aus 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Value Iteration
\end_layout

\begin_layout Subsection
Policy Extraction
\end_layout

\begin_layout Section
Reinforcement Learning
\end_layout

\begin_layout Subsection
Verbindung zu MDPs
\end_layout

\begin_layout Subsection
Verschiedene Ansätze
\end_layout

\begin_layout Subsection
Q-Values/Q-Learning
\end_layout

\begin_layout Subsection
Exploration
\end_layout

\begin_layout Subsection
State Features
\end_layout

\begin_layout Subsection
Deep Q-Learning
\end_layout

\begin_layout Section
Fazit
\end_layout

\begin_layout Section
Quellen
\end_layout

\end_body
\end_document
