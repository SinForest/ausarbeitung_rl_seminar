#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 3cm
\rightmargin 2.5cm
\bottommargin 3.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Einführung in das Reinforcement Learning
\end_layout

\begin_layout Author
Patrick Dammann
\end_layout

\begin_layout Date
heute
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Abstract
\end_layout

\begin_layout Standard
Diese schriftlichen Ausarbeitung zu meinem Seminar-Vortrag mit dem Thema
 
\begin_inset Quotes eld
\end_inset

Einführung in das Reinforcement Learning
\begin_inset Quotes erd
\end_inset

 soll einen kurzen Überblick über das Thema Reinforcement Learning im Allgemeine
n geben.
\end_layout

\begin_layout Standard
Der erste Teil erklärt dafür zunächst die generelle Problemstellung, für
 die Reinforcement Learning eine Lösung anbieten möchte.
 Hierbei soll erklärt werden, was ein Markov Entscheidungs-Problem ist,
 welche Notationen dafür benötigt werden und wie man es bei Vorhandensein
 des vollständigen Modells lösen kann.
\end_layout

\begin_layout Standard
Der zweite Teil der Ausarbeitung stellt Reinforcement Learning als Konzept
 vor, jede Probleme zu lösen wenn nicht alle Informationen direkt verfügbar
 sind.
 Hierbei werden hauptsächlich grundlegende Ideen von Algorithmen so wie
 Verbesserungen für eben jene vorgestellt, und zuletzt die aktuelle Relevanz
 des Konzepts bedingt durch Integrationsmöglichkeit von state-of-the-art-Methode
n aufgezeigt.
\end_layout

\begin_layout Section
Markov Entscheidungs-Probleme
\end_layout

\begin_layout Standard
Markov Entscheidungs-Probleme (kurz 
\series bold
MDP
\series default
 für 
\emph on
markov decision process
\emph default
) sind ein Modell für Probleme, bei denen ein Agent versucht die größtmögliche
 Belohnung (
\emph on
reward
\emph default
) zu erzielen.
 Er bewegt sich dazu durch eine Menge von Zuständen (
\emph on
states
\emph default
), indem er aus einer Menge von Aktionen (
\emph on
actions
\emph default
) wählt.
 Welchen Zustand der Agent erreicht ist nicht deterministisch, die Wahrscheinlic
hkeiten sind jedoch nur von der gewählten Aktion und dem aktuellen Zustand
 abhängig.
 Die Belohnung, die der Agent für einen Übergang erhält, wird durch Ausgangs-
 und Endzustand des Übergangs so wie die gewählte Aktion bestimmt.
 Der Agent startet in einem Startzustand, es kann diverse Endzustände geben,
 nach deren Erreichen keine Aktionen mehr ausgeführt werden können.
\end_layout

\begin_layout Standard
Ziel ist es, eine Strategie (
\emph on
policy
\emph default
) zu finden, welche jedem Zustand die Aktion zuordnet, durch die der höchste
 Gesamtgewinn (also die höchste Summe über alle erzielten Gewinne) erwartet
 werden kann.
\end_layout

\begin_layout Subsection
Ein Beispiel
\end_layout

\begin_layout Standard
In diesem Beispiel werden die Zustände durch die Felder eines 
\begin_inset Formula $4\times4$
\end_inset

-Grids visualisiert.
 Der Startzustand 
\begin_inset Formula $\left(3,0\right)$
\end_inset

 ist durch ein großes 
\begin_inset Formula $S$
\end_inset

 markiert, die Zustände mit einem 
\begin_inset Formula $X$
\end_inset

 sind nicht erreichbar.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename media/example.svg
	lyxscale 15
	scale 35

\end_inset


\end_layout

\begin_layout Standard
Die Menge der Aktionen besteht aus 4 Elementen, jedes symbolisiert einen
 Schritt in eine der vier Himmelsrichtungen.
 Beim Ausführen einer Aktion landet der Agent mit einer Chance von 
\begin_inset Formula $0.8$
\end_inset

 ein Feld weiter in der gewählten Himmelsrichtung, so wie mit je einer Chance
 von 
\begin_inset Formula $0.1$
\end_inset

 in einer der zwei orthogonalen Richtungen.
 Würde der Agent hierbei aif einem Feld landen, welches er nicht betreten
 kann (eines der mit 
\begin_inset Formula $X$
\end_inset

 markierten, oder außerhalb des Grids), bleibt er auf seinem Feld.
\end_layout

\begin_layout Standard
Von den bunten Feldern aus führt jede Aktion mit einer Chance von 
\begin_inset Formula $1$
\end_inset

 in den Endzustand.
 Der Agent erhält für diesen Übergang die auf dem Feld verzeichnete Belohnung
 (100 oder -100).
 Für jede andere Aktion wird eine Belohnung von -2 verbucht.
\end_layout

\begin_layout Subsection
Notation
\end_layout

\begin_layout Standard
Zur Beschreibung eines MDPs werden einige Notationen benötigt, die hier
 angegeben werden.
 
\emph on
(Zu einigen Notationen werden Beispiele zum oben genannten Beispiel angegeben.)
\emph default
 Zur Problemstellung selbst gehören:
\end_layout

\begin_layout Itemize
\begin_inset Formula $S=\left\{ s_{1},\ldots,s_{n}\right\} $
\end_inset

 ist die Menge der 
\series bold
Zustände
\series default
.
 Die Elemente der Menge können beliebig benannt werden.
\end_layout

\begin_layout Itemize
\begin_inset Formula $A=\left\{ a_{1},\ldots,a_{n}\right\} $
\end_inset

 ist die Menge der 
\series bold
Aktionen
\series default
.
 Auch diese Elemente können beliebig benannt werden.
\end_layout

\begin_layout Itemize
\begin_inset Formula $T:S\times A\times S\rightarrow\left[0,1\right]$
\end_inset

 ist die 
\series bold
Übergangs-Funktion
\series default
 (
\emph on
transition function
\emph default
).
 
\begin_inset Formula $T\left(s,a,s^{\prime}\right)$
\end_inset

 beschreibt die Wahrscheinlichkeit, mit der der Agent von Zustand 
\begin_inset Formula $s$
\end_inset

 in den Zustand 
\begin_inset Formula $s^{\prime}$
\end_inset

 wechselt, wenn er Aktion 
\begin_inset Formula $a$
\end_inset

 ausführt.
 Eine allgemeinere, jedoch hier nicht verwendete Schreibweise wäre 
\begin_inset Formula $T\left(s,a,s^{\prime}\right)=p\left(s^{\prime}|s,a\right)$
\end_inset

.
\begin_inset Formula 
\begin{align*}
T(s_{0,2},a_{E},s_{0,3}) & =0.8 & \text{(Agent wählt Osten und kommt dort an)}\\
T(s_{0,2},a_{E},s_{0,2}) & =0.1 & \text{(Agent wählt Osten, geht nach Norden, kein valider Zug)}\\
T(s_{0,2},a_{E},s_{1,2}) & =0.1 & \text{(Agent wählt Osten, geht nach Süden)}\\
T(s_{0,2},a_{E},s_{0,1}) & =0.0 & \text{(Agent wählt Osten, Bewegung nach Westen unmöglich)}\\
T(s_{1,2},\_,s_{end}) & =1.0 & \text{(jede Aktion von bunten Feldern führt zum Endzustand)}
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $R:S\times A\times S\rightarrow\mathbb{R}$
\end_inset

 ist die 
\series bold
Belohnungs-Funktion
\series default
 (
\emph on
reward function
\emph default
).
 
\begin_inset Formula $R\left(s,a,s^{\prime}\right)$
\end_inset

 ordnet dem Übergang von Zustand 
\begin_inset Formula $s$
\end_inset

 nach Zustand 
\begin_inset Formula $s^{\prime}$
\end_inset

 mit Hilfe von Aktion 
\begin_inset Formula $a$
\end_inset

 eine Belohung zu.
\begin_inset Formula 
\begin{align*}
R(s_{1,2},\_,s_{end}) & =-100 & \text{(Belohnung von buntem Feld)}\\
R(s_{1,3},\_,s_{end}) & =+100 & \text{(Belohnung von buntem Feld)}\\
R(s_{0,2},a_{N},s_{0,1}) & =-2 & \text{(jede Bewegung kostet 2 Belohung)}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Zum Lösen der Probleme werden folgende Notationen verwendet:
\end_layout

\begin_layout Itemize
\begin_inset Formula $\pi\left(s\right)$
\end_inset

 ist eine 
\series bold
Policy
\series default
, die dem Zustand 
\begin_inset Formula $s$
\end_inset

 die optimale Aktion zuordnet.
 Hierbei beschreibt 
\begin_inset Formula $\pi$
\end_inset

 eine Policy im Allgemeinen, 
\begin_inset Formula $\pi^{\ast}$
\end_inset

 die optimale Policy.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\gamma\in\left[0,1\right]$
\end_inset

 ist der sogenannte 
\series bold
Discount-Faktor
\series default
.
 Für den Agenten werden nach jeder Aktion alle noch erreichbaren Belohnungen
 mit 
\begin_inset Formula $\gamma$
\end_inset

 multipliziert, damit er kürzere Wege, die zur selben Belohnung führen,
 bevorzugen wird.
 Mit 
\begin_inset Formula $\gamma=1$
\end_inset

 ist für den Agenten ein langer Weg ebenso gut wie ein kurzer, solange er
 die selbe Belohnung erhält.
 Mit 
\begin_inset Formula $\gamma=0$
\end_inset

 wird der Agent nicht mehr vorausschauend planen, da für ihn nur die nächste
 Belohnung Wert hat.
 Es gilt also, einen geeigneten Trade-Off zu finden.
\end_layout

\begin_layout Subsection
V-Values
\end_layout

\begin_layout Standard
Um ein MDP zu lösen, lässt sich auf ein Bewertungsschema für Zustände zurückgrei
fen.
 Hierbei ergibt sich die Bewertung eines Zustandes aus der 
\series bold
Gesamtbelohung, die der Agent zu erwarteten hat, wenn er eine Simulation
 in besagtem Zustand startet, die optimale Aktion ausführt und von dort
 an optimal handelt
\series default
.
 Die Markov-Bedingung erlaubt uns in jedem Zustand anzunehmen, wir hätten
 die Simulation gerade erst gestartet, da es keine Faktoren gibt, die von
 bereits vergangenen Ereignissen abhängen.
\end_layout

\begin_layout Standard
Da die Bewertung jedes Zustandes angibt, welche Belohnung von ihm aus zu
 erwarten ist, lässt sich die Bewertung wie folgt rekursiv definieren:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
V^{\ast}\left(s\right)=\max_{a\in A}\sum_{s^{\prime}\in S}T\left(s,a,s^{\prime}\right)\left[R\left(s,a,s^{\prime}\right)+\gamma\cdot V^{\ast}\left(s^{\prime}\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
In der eckigen Klammer befindet sich die unmittelbare Belohung, wenn der
 Agent von Zustand 
\begin_inset Formula $s$
\end_inset

 über Aktion 
\begin_inset Formula $a$
\end_inset

 in Zustand 
\begin_inset Formula $s^{\prime}$
\end_inset

 gelandet ist, addiert zur zu erwartenden Belohnung von Zustand 
\begin_inset Formula $s^{\prime}$
\end_inset

 aus (welche mit 
\begin_inset Formula $\gamma$
\end_inset

 multipliziert wurde, da eine Aktion vorrüber ist).
\end_layout

\begin_layout Standard
Dieser Wert wird mit der Wahrscheinlichkeit des Auftretens dieses Übergangs
 multipliziert und durch Aufsummieren über alle möglichen Zustände 
\begin_inset Formula $s^{\prime}$
\end_inset

 (
\emph on
man bedenke, dass 
\begin_inset Formula $T$
\end_inset

 nur für Zustände, die von 
\begin_inset Formula $s$
\end_inset

 über 
\begin_inset Formula $a$
\end_inset

 erreicht werden können 
\begin_inset Formula $>0$
\end_inset

 ist
\emph default
) zum Erwartungswert für eine Aktion 
\begin_inset Formula $a$
\end_inset

.
\end_layout

\begin_layout Standard
Da 
\begin_inset Formula $V^{\ast}$
\end_inset

 als erwartete Belohnung definiert ist, wenn der Agent immer optimal handelt,
 ist es der Erwartungswert der Aktion, von welcher die höchste Belohnung
 erwartet wird.
\end_layout

\begin_layout Subsection
Value Iteration
\end_layout

\begin_layout Standard
Durch die rekursive Definition von 
\begin_inset Formula $V^{\ast}$
\end_inset

 lässt es sich nicht trivial berechnen.
 Als möglicher Lösungsansatz bietet sich daher die Value Iteration an.
 Hierzu wird das MDP zeitlich begrenzt und 
\begin_inset Formula $V^{\left(k\right)}$
\end_inset

 definiert als 
\emph on
Gesamtbelohung, die der Agent zu erwarteten hat, wenn er eine Simulation
 in besagtem Zustand startet, die optimale Aktion ausführt und von dort
 an optimal handelt, 
\series bold
aber nur noch 
\begin_inset Formula $k$
\end_inset

 Schritte übrig hat
\series default
\emph default
.
\end_layout

\begin_layout Standard
\begin_inset Formula $V^{\left(0\right)}$
\end_inset

 lässt sich einfach bestimmen, da es 
\begin_inset Formula $0$
\end_inset

 für jeden Zustand sein muss.
 Auch 
\begin_inset Formula $V^{\left(1\right)}$
\end_inset

 folgt oft direkt aus der Problemdefinition.
 In unserem Beispiel wäre es 
\begin_inset Formula $-100$
\end_inset

 bzw 
\begin_inset Formula $+100$
\end_inset

 für die 
\begin_inset Quotes eld
\end_inset

bunten
\begin_inset Quotes erd
\end_inset

 Zustände und 
\begin_inset Formula $-2$
\end_inset

 für alle anderen, da jeder Schritt 
\begin_inset Formula $-2$
\end_inset

 Belohnung bringt.
\end_layout

\begin_layout Standard
Ein beliebiges 
\begin_inset Formula $V^{\left(k\right)}$
\end_inset

 lässt sich einfach berechnen.
 Dazu muss nur die Formel für 
\begin_inset Formula $V^{\ast}$
\end_inset

leicht angepasst werden:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
V^{\left(k\right)}\left(s\right)=\max_{a\in A}\sum_{s^{\prime}\in S}T\left(s,a,s^{\prime}\right)\left[R\left(s,a,s^{\prime}\right)+\gamma\cdot V^{\left(k-1\right)}\left(s^{\prime}\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Da sich das hintere 
\begin_inset Formula $V^{\ast}$
\end_inset

 in der Gleichung ja auf Geschehnisse bezog, die einen Zeitschritt später
 passieren, muss es bei der Berechnung von 
\begin_inset Formula $V^{\left(k\right)}$
\end_inset

 durch ein 
\begin_inset Formula $V^{\left(k-1\right)}$
\end_inset

 ersetzt werden, da der Agent im zeitlich beschränkten MDP einen Zeitschritt
 später selbstverständlich nur noch einen Zeitschritt weniger zur Verfügung
 hat.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
kürzbar, wenn zu viel
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename media/example_value_iteration.svg
	lyxscale 20
	width 95text%

\end_inset


\end_layout

\begin_layout Standard
Für MDPs, deren maximal erzielbare Belohung nach oben beschränkt ist, wird
 
\begin_inset Formula $V^{\left(k\right)}$
\end_inset

 für 
\begin_inset Formula $k\longrightarrow\infty$
\end_inset

 konvergieren, da sich auch mit mehr Schritten kein besseres Ergebnis mehr
 erzielen lassen wird.
 Durch ein 
\begin_inset Formula $\gamma<1$
\end_inset

 kann quasi eine Beschränkung simuliert werden.
\end_layout

\begin_layout Standard
Für ein geeignet großes 
\begin_inset Formula $k$
\end_inset

 gilt also 
\begin_inset Formula $V^{\left(k\right)}=V^{\ast}\pm\varepsilon$
\end_inset

 mit vernachlässigbarem 
\begin_inset Formula $\varepsilon$
\end_inset

 (
\emph on
z.B.
 kleiner als Maschinengenauigkeit oder kleiner als eine vorher gewählte
 Toleranz
\emph default
), da 
\begin_inset Formula $V^{\left(\infty\right)}$
\end_inset

 genau unserer Definition für ein 
\begin_inset Formula $V^{\ast}$
\end_inset

 im nicht zeitlich beschränkten MDP entspricht.
\end_layout

\begin_layout Subsection
Policy Extraction
\end_layout

\begin_layout Standard
Mit Hilfe der optimalen V-Values 
\begin_inset Formula $V^{\ast}$
\end_inset

 lässt sich jetzt eine optimale Policy für das MDP berechnen.
 Dazu simuliert man einen weiteren Schritt der Value Iteration.
 Anstatt nun aber tatsächlich das Maximum über alle 
\begin_inset Formula $a$
\end_inset

 zu bestimmen, weisen wir dem aktullen Zustand die Aktion zu, die von ihm
 aus die erwartete Belohnung maximieren würde.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\pi^{*}\left(s\right)=\underset{a\in A}{\textrm{argmax}}\sum_{s^{\prime}\in S}T\left(s,a,s^{\prime}\right)\left[R\left(s,a,s^{\prime}\right)+\gamma\cdot V^{\ast}\left(s^{\prime}\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Auf diese Weise haben wir eine optimale Strategie erzeugt und somit das
 MDP gelöst.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename media/example_policy_extraction.svg
	lyxscale 20
	width 62text%

\end_inset


\end_layout

\begin_layout Section
Reinforcement Learning
\end_layout

\begin_layout Standard
Reinforcement Learning ist ein Machine-Learning-Konzept, in welchem ein
 Agent ohne anfängliche Informationen über sein Umfeld oder die Auswirkungen
 seines Handelns dazu trainiert werden soll, die maximale Belohnung zu erhalten.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
mehr? mit 3.1 zusammenlegen?
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Verbindung zu MDPs
\end_layout

\begin_layout Standard
Während sich zum Beispiel mit Value Iteration und Policy Extraction MDPs
 lösen lassen, bei denen alle Informationen vorhanden sind, bietet Reinforcement
 Learning eine Reihe von Algorithmen bei welchen vom Agenten keine Kenntnis
 von 
\begin_inset Formula $T\left(s,a,s^{\prime}\right)$
\end_inset

 oder 
\begin_inset Formula $R\left(s,a,s^{\prime}\right)$
\end_inset

 vorrausgesetzt werden.
 Der Agent muss sich also selbstständig über Aktionen durch die Zustände
 bewegen, um die verschiedenen Übergänge und deren Belohnungen und Wahrscheinlic
hkeiten selbst zu erfahren.
\end_layout

\begin_layout Standard
Da der Agent erst nach jedem Übergang die erhaltene Belohnung von extern
 mitgeteilt bekommt, lassen sich so bei der Problemmodellierung einfache
 Möglichkeiten implementieren, Belohnungen zu verteilen.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Beispiel?
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Model Based Approaches
\end_layout

\begin_layout Standard
Die erste Möglichkeit an dieses Problem heranzugehen wäre ein 
\series bold
model based approach
\series default
.
 Das bedeutet, der Agent versucht zunächst, die fehlenden Teile des Modells
 (also vor allem 
\begin_inset Formula $T$
\end_inset

 und 
\begin_inset Formula $R$
\end_inset

) durch Samples zu approximieren.
 Sobald diese annährend bekannt sind, lässt sich das Problem mit beliebigen
 MDP-Algorithmen (wie zum Beispiel Value Iteration und Policy Extraction)
 lösen.
 
\end_layout

\begin_layout Standard
Dieser Ansatz erfordert sehr große Mengen an Samples welche zur Auswertung
 gleichzeitig zur Verfügung stehen müssen.
 Ausserdem erfordert das Sammeln der Samples einen großen Grad an Freiheit
 in der Bewegung des Agenten durch die Zustände, was zum Beispiel bei Anwendunge
n in der Robotik oft nicht gegeben ist.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Beispiel? Fußnote?
\end_layout

\end_inset

 Bei sehr großen (oder sogar indiskreten) Zustands-Mengen ist dieser Ansatz
 also nur schwer umsetzbar.
\end_layout

\begin_layout Subsection
Temporal Difference Learning
\end_layout

\begin_layout Standard
Als weiterer Ansatz bietet sich das 
\series bold
temporal difference learning
\series default
 an.
 Die Idee dahinter ist, dass mit zufällig gewählten V-Values begonnen wird
 und diese langsam iterativ verbessert werden.
\end_layout

\begin_layout Standard
Nach jeder Aktion (und deren Feedback) überprüft der Agent, wie sich die
 ihm über V-Values versprochene erwartete Belohunung im Vergleich zur direkt
 erhaltenen Belohung und der nun in Aussicht stehenden erwarteten Belohnung
 verhält und passt die V-Values des eben verlassenen Zustandes geringfügig
 dementsprechend an.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
V\left(s\right)\leftarrow V\left(s\right)+\alpha\left[r+\gamma V\left(s^{\prime}\right)-V\left(s\right)\right]
\]

\end_inset

 Die Bewertung von Zustand 
\begin_inset Formula $s$
\end_inset

 wird um einen kleinen Teil 
\begin_inset Formula $\left(\alpha<0\right)$
\end_inset

 der Differenz zwischen versprochener 
\begin_inset Formula $\left(V\left(s\right)\right)$
\end_inset

 und erfahrener 
\begin_inset Formula $\left(r+\gamma V\left(s^{\prime}\right)\right)$
\end_inset

 Belohung angepasst.
 Die Lernrate (
\emph on
learning rate
\emph default
) 
\begin_inset Formula $\alpha$
\end_inset

 ist dabei ein wichtiger Hyperparameter und entscheident für die Konvergenz
 des Verfahrens.
 Für ein sich verringerndes 
\begin_inset Formula $\alpha_{i}$
\end_inset

 (
\begin_inset Formula $\alpha$
\end_inset

 im 
\begin_inset Formula $i$
\end_inset

-ten Zeitschritt) mit 
\begin_inset Formula $\sum a_{i}=\infty$
\end_inset

 und 
\begin_inset Formula $\sum a_{i}^{2}<\infty$
\end_inset

 konvergiert das Verfahren in jedem Fall gegen die optimale Lösung (sofern
 es eine gibt).
 Aber auch für konstante, sehr kleine 
\begin_inset Formula $\alpha$
\end_inset

 konvergiert das Verfahren in der Praxis sehr oft.
\end_layout

\begin_layout Standard
Oft lässt sich der Rechenaufwand für das Lernen auch verkleinern, in dem
 beim Erkunden der Umgebung zunächst viele Samples gesammelt wurden und
 das Lernen danach auf allen gesammelten Samples am Stück passiert.
 Erst danach wird die Policy aktualisiert, damit wieder neue Samples gesammelt
 werden können.
 Dies wird bis zur Konvergenz wiederholt.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Probleme mit Policy?
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Q-Values/Q-Learning
\end_layout

\begin_layout Standard
Das explizite Aktualisieren der Policy kann entfallen, wenn die Policy direkt
 implizit mitgelernt wird.
 Diese Idee kann durch die sogenannten Q-Values umgesetzt werden.
 Hierbei werden statt einem Wert pro State 
\begin_inset Formula $\left|A\right|$
\end_inset

 Werte pro State gespeichert, jeweils einer pro ausführbarer Aktion.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename media/example_q_values.svg
	lyxscale 15
	scale 25

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Es ist also 
\begin_inset Formula $Q\left(s,a\right)$
\end_inset

 definiert als 
\emph on
Gesamtbelohung, die der Agent zu erwarten hat, wenn er eine Simulation in
 besagtem Zustand startet, 
\series bold
Aktion 
\begin_inset Formula $\mathbf{a}$
\end_inset


\series default
 
\series bold
\emph default
ausführt
\series default
\emph on
 und von dort an optimal handelt
\emph default
, was formal bedeutet:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Q^{\ast}\left(s,a\right)=\sum_{s^{\prime}\in S}T\left(s,a,s^{\prime}\right)\left[R\left(s,a,s^{\prime}\right)+\gamma\cdot V^{\ast}\left(s^{\prime}\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Hierbei lässt sich sofort erkennen, dass sowohl die V-Values als auch die
 gewünschte Startegie implizit mitgeliefert wird:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
V\left(s\right) & =\underset{a\in A}{\textrm{max }}Q\left(s,a\right)\\
\pi\left(s\right) & =\underset{a\in A}{\textrm{argmax }}Q\left(s,a\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Viele beliebte Reinforcement-Learning-Algorithmen basieren auf dem direkten
 Lernen der Q-Values.
 Einer der einfachsten ist hierbei 
\series bold
Q-Learning
\series default
, welcher nur eine Form von Temporal Difference Learning auf Q-Values darstellt.
 Die Update-Regel wird dabei lediglich wie folgt abgeändert:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Q\left(s,a\right)\leftarrow Q\left(s,a\right)+\alpha\left[r+\gamma\max_{a^{\prime}\in A}Q\left(s^{\prime},a^{\prime}\right)-Q\left(s,a\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Selbstverständlich benötigt Q-Learning aber auch mehr Samples zum konvergieren,
 da 
\begin_inset Formula $\left|A\right|$
\end_inset

-mal so viele Parameter gelernt werden.
\end_layout

\begin_layout Subsection
Exploration
\end_layout

\begin_layout Standard
Während der Agent die Zustände erkundet, hat er stets die Auswahl zwischen
 zwei generellen Lernstrategien.
 Er muss entscheiden, ob er bisher schlecht erkundete Wege durch den Zustandsrau
m besuchen möchte um mehr über sie herauszufinden, oder ob er sich an seine
 aktuelle Strategie hält um ihre Richtigkeit zu überprüfen und vielleicht
 weniger erforschte Zustände zu erforschen, die er erst durch bereits für
 gut befundene Entscheidungen erreicht.
\end_layout

\begin_layout Standard
Es gibt viele Methoden um einen geeigneten Trade-Off zwischen den beiden
 Extrema zu finden.
 Drei davon, welche auch beliebig untereinander kombiniert werden können,
 werden hier vorgestellt:
\end_layout

\begin_layout Subsubsection
Startbedingungen
\end_layout

\begin_layout Standard
Zu Beginn vieler Reinforcement-Learning-Algorithmen werden Startwerte zufällig
 initialisiert.
 Wenn zum Beispiel die Q-Values allesamt höher initialisiert werden als
 der höchte Wert, von dem man glaubt, dass er von ihnen angenommen werden
 kann, so werden die ersten Updates sehr häufig zu Verringerungen der Werte
 führen.
 Dadurch ist der Agent in den ersten Iterationen eher geneigt Wege auszuprobiere
n, für die er noch keine Updates durchgeführt hat.
\end_layout

\begin_layout Standard
Selbstverständlich ist diese Technik nur in den ersten Interationen sinnvoll
 und sollte mit anderen Methoden kombiniert werden.
\end_layout

\begin_layout Subsubsection
Die 
\begin_inset Formula $\epsilon$
\end_inset

-greedy Exploration
\end_layout

\begin_layout Standard
Bei dieser Erkundungs-Technik wird der neue, namensgebende Hyperparameter
 
\begin_inset Formula $0\leq\epsilon\leq1$
\end_inset

 eingeführt.
 Vor jeder Aktion wirft der Agent nun eine unfaire Münze, so dass er mit
 einer Chance von 
\begin_inset Formula $\left(1-\epsilon\right)$
\end_inset

 nach seiner Strategie handelt, jedoch mit einer Chance von 
\begin_inset Formula $\epsilon$
\end_inset

 eine völlig zufällige Aktion durchführt.
\end_layout

\begin_layout Standard
Während 
\begin_inset Formula $\epsilon$
\end_inset

 zur Test-Zeit für gewöhnlich auf 
\begin_inset Formula $0$
\end_inset

 gesetzt wird, kann es zur Trainingszeit je nach Bedarf konstant sein, mit
 der Zeit abnehmen oder anderweitig an die Gegebenheiten angepasst werden.
\end_layout

\begin_layout Subsubsection
Erkundungs-Funktionen
\end_layout

\begin_layout Standard
Um die Erkundung tatsächlich gezielt in unbekannte Regionen zu lenken, lässt
 sich der Iterationsschritt in der Berechnung der Q-Values wie folgt anpassen:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Q\left(s,a\right)\leftarrow Q\left(s,a\right)+\alpha\left[r+\gamma\max_{a^{\prime}\in A}\left(Q\left(s^{\prime},a^{\prime}\right)+E\left(s^{\prime},a^{\prime}\right)\right)-Q\left(s,a\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Die Funktion 
\begin_inset Formula $E\left(s^{\prime},a^{\prime}\right)$
\end_inset

 sinkt dabei jedes Mal, wenn die Aktion 
\begin_inset Formula $a^{\prime}$
\end_inset

 im Zustand 
\begin_inset Formula $s^{\prime}$
\end_inset

 ausgeführt wird.
 Sie kann zum Beispiel die Form 
\begin_inset Formula $E\left(s,a\right)=\nicefrac{k}{n_{s,a}}$
\end_inset

 haben, wobei 
\begin_inset Formula $n_{s,a}$
\end_inset

 die Anzahl der Vorkommnisse des Zustand-Aktion-Tupels 
\begin_inset Formula $\left(s,a\right)$
\end_inset

 ist.
\end_layout

\begin_layout Standard
Da 
\begin_inset Formula $E\left(s,a\right)$
\end_inset

 gegen 
\begin_inset Formula $0$
\end_inset

 konvergiert, und fälschlicherweise gut klassifizierte Aktionen oft gewählt
 und im Wert verringert werden, sollte beim Konvergieren der Q-Learing-Methode
 der Wert der Erkundungsfunktion keinen merklichen Einfluss mehr auf die
 gelernte Strategie haben.
\end_layout

\begin_layout Subsection
State Features
\end_layout

\begin_layout Standard
Gerade in realistischen Anwendungen wird die Anzahl verschiedener Zustände
 schnell unüberschaubar groß.
 Eine ungefähre Vorstellung lässt sich über diese grobe, obere Abschätzung
 der möglichen Zustände eines Flash-Spiels vermitteln:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename media/example_state_space.svg
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Standard
Man stelle sich ein Szenario vor, in dem der Reinforcement-Learning-Agent
 lernen soll, ein einfaches Spiel im Stil des bekannten Videospiels 
\begin_inset Quotes eld
\end_inset

Bomberman
\begin_inset Quotes erd
\end_inset

 zu gewinnen.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Kurze Spiel-Erklärung?
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
Das Spielfeld in diesem Beispiel besteht aus 
\begin_inset Formula $11\cdot13$
\end_inset

 Feldern, welche jeweils 15 verschiedene Zustände annehmen können.
 Diese Zustände sind: leer, zerstörbarer Stein, unzerstörbarer Stein, Feuer
 von je einem der 4 Spieler, Dynamit von je einem der 4 Spieler oder eines
 von 4 sammelbaren Power-Ups.
\end_layout

\begin_layout Standard
Allein dadurch gibt es schon 
\begin_inset Formula $15^{11\cdot13}\approx1.5\cdot10^{168}$
\end_inset

 verschiedene Zustände, bereits mehr als das Quadrat aller Atome im Universum.
\end_layout

\begin_layout Standard
Wenn man jetzt noch betrachtet, dass in jedem dieser Zustände jeder der
 vier Spieler auf jedem der 
\begin_inset Formula $44\cdot44$
\end_inset

 Pixel jedes Feldes stehen könnte und möglicherweise die Zeit, die auf dem
 Timer auf die Sekunde genau abgelesen werden kann, die Spielweise des Agenten
 beeinflussen sollte und somit neue Zustände generiert, kommt man sogar
 auf eine Kardinalität von 
\begin_inset Formula $15^{11\cdot13}\cdot\left(13\cdot11\cdot44^{2}\right)^{4}\cdot\left(3\cdot60\right)\approx1.6\cdot10^{192}$
\end_inset

.
\end_layout

\begin_layout Standard
Natürlich ist dies nur eine obere Abschätzung, da einige der Zustände sicherlich
 unerreichbar sind.
 Die ungefähre Größenordnung zeigt jedoch gut auf, wie schnell die Zustandsmenge
 unermesslich groß werden kann.
\end_layout

\begin_layout Standard
Nicht nur, dass der Agent für jeden dieser Zustände eine eigene Aktion für
 die Strategie bzw.
 einen Q-Value lernen muss, er kann auch nicht über verschiedene Zustände
 hinweg abstrahieren und somit von Gelerntem über Zustände auf ähnliche
 Zustände schließen.
\end_layout

\begin_layout Standard
Aus diesem Grund lohnt es sich, statt eines Lookup-Tables für die Q-Values
 eine Q-Funktion zu approximieren.
 Eine Möglichkeit hierfür ist, nur bestimmte Features aus den Zuständen
 zu extrahieren und mit diesen über eine gewichtete Summe die Q-Values anzunäher
en.
 Dies könnte beispielsweise diese Form haben:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Q\left(s,a\right)=w_{1}f_{1}\left(s,a\right)+w_{2}f_{2}\left(s,a\right)+\ldots+w_{n}f_{n}\left(s,a\right)
\]

\end_inset


\end_layout

\begin_layout Standard
An Stelle der Q-Values selbst werden nun die Gewichte zu ihrer Berechnung
 gelernt.
 Ein Update nachdem in Zustand 
\begin_inset Formula $s$
\end_inset

 Aktion 
\begin_inset Formula $a$
\end_inset

 ausgeführt wurde, könnte dabei so aussehen:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
w_{i}\leftarrow w_{i}+\alpha\left[r+\gamma\max_{a^{\prime}\in A}Q\left(s^{\prime},a^{\prime}\right)-Q\left(s,a\right)\right]\cdot f_{i}\left(s,a\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Natürlich hängt hierbei der Erfolg der Methode sehr stark von den gewählten
 Features ab.
\end_layout

\begin_layout Subsection
Andere Algorithmen
\end_layout

\begin_layout Standard
Ausser naivem Q-Learning gibt es noch viele Verbesserungen am generellen
 Algorithmus, von denen zwei hier vorgestellt werden sollen:
\end_layout

\begin_layout Subsubsection
SARSA
\end_layout

\begin_layout Standard
Der SARSA-Algorithmus (kurz für State-Action-Reward-State-Action) ist eine
 Abwandlung von Q-Learning, die statt des Q-Values für die optimale nächste
 Aktion den Wert der tatsächlich als nächstes ausgeführten Aktion nutzt,
 um in den Updates den nächsten Zustand zu bewerten.
\end_layout

\begin_layout Standard
Da der Einfluss von zum Beispiel Exploration Strategien für gewöhnlich gegen
 Ende des Trainings gegen null geht, weicht der Agent dann nur noch sehr
 selten von der für ihn optimalen Policy ab, wodurch der Unterschied zu
 Q-Learning gering wird und SARSA eine optimale Policy lernt.
\end_layout

\begin_layout Subsubsection
Double Q-Learning
\end_layout

\begin_layout Standard
Q-Learning tendiert dazu, unter gewissen Umständen den Wert von Zustand-Aktions-
Paaren zu überschätzen.
 Dies kommt unter anderem daher, dass sich durch das Suchen des Maximums
 in der Update-Regel Überschätzungen von einzelnen Werten durch viele Updates
 ziehen kann.
\end_layout

\begin_layout Standard
Um die Idee von Q-Learning zu skizzieren, sollte man sich folgende alternative
 Schreibweise der Update-Rule anschauen:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Q\left(s,a\right)\leftarrow Q\left(s,a\right)+\alpha\left[r+\gamma Q\left(s^{\prime},\underset{a^{\prime}\in A}{\text{argmax }}Q\left(s^{\prime},a^{\prime}\right)\right)-Q\left(s,a\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
In Double Q-Learning werden nun zwei Q-Functions gelernt.
 In jedem Update wird dann eine von beiden zufällig ausgewählt und bekommt
 den Namen 
\begin_inset Formula $Q_{1}$
\end_inset

, die andere heißt 
\begin_inset Formula $Q_{2}$
\end_inset

.
 
\begin_inset Formula $Q_{1}$
\end_inset

 ist die Funktion, die in diesem Schritt geupdatet wird.
 Über sie wird auch ermittelt, welche Aktion im nächsten Schritt optimal
 wäre.
 Der Wert der optimalen Aktion wird jedoch aus 
\begin_inset Formula $Q_{2}$
\end_inset

 ausgelesen.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Q_{1}\left(s,a\right)\leftarrow Q_{1}\left(s,a\right)+\alpha\left[r+\gamma Q_{2}\left(s^{\prime},\underset{a^{\prime}\in A}{\text{argmax }}Q_{1}\left(s^{\prime},a^{\prime}\right)\right)-Q_{1}\left(s,a\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Wichtig hierbei ist, dass die Zuweisung von 
\begin_inset Formula $Q_{1}$
\end_inset

 und 
\begin_inset Formula $Q_{2}$
\end_inset

 vor jedem Update (oder zumindest regelmäßig) geschieht, damit beide Funktionen
 gleichmäßig aktualisiert werden.
\end_layout

\begin_layout Subsection
Deep Q-Learning
\end_layout

\begin_layout Standard
Um gut und schnell Q-Values zu lernen benötigt man eine starke und einfach
 zu trainierende Methode zur Funktionsapproximierung, da bei realistischen
 Kardinalitäten der Zustands-Menge keine Lookup-Tables benutzt werden können.
 Häufig scheitern Versuche aber auch daran, dass von Hand extrahierte Features
 eines Zustandes dem Agenten nicht die Informationen zukommen lassen, die
 er für eine optimale Entscheidungsfindung benötigen würde.
\end_layout

\begin_layout Standard
In den letzten Jahren haben sich neuronale Netze mehr und mehr als Konzept
 der Wahl in beiden Problemfällen erwiesen.
 Sie können beliebige Funktionen approximieren, lassen sich gut und sehr
 einfach auf GPUs trainieren und lernen überraschend präzise Features zu
 extrahieren, vor allem wenn diese im Rest des Netzwerks weiterverarbeitet
 werden.
 Als kleinen Bonus haben sich in den letzten Jahren Netzwerk-Architekturen
 für etliche Problemfälle etabliert, weshalb es egal ist, ob die Umgebung
 dem Agenten den Zustand in Form von Zahlen, Bildern (Video-Kamera, Video-Spiel)
 oder sogar Audiospuren oder Text zukommen lässt.
\end_layout

\begin_layout Standard
Das Neuronale Netz bekommt beim Deep Q-Learning sämtliche Informationen
 der Umgebung als Input, möglicherweise zusätzlich sogar zwischengespeicherte
 Zusatzinfomationen wie die Aktionen und Zustände der letzten 
\begin_inset Formula $n$
\end_inset

 Aktionen.
 Als Output wird ein 
\begin_inset Formula $\left|A\right|$
\end_inset

-dimensionaler Vektor erwartet, der die Q-Values zu allen Aktionen beinhaltet.
\end_layout

\begin_layout Standard
Da Neuronale Netze zum Trainieren ein Target brauchen, also ein Ergebnis,
 das der Trainierende statt des tatsächlichen Ergebnisses gewünscht hätte,
 wird als Target genau das definiert, was im Kapitel über Temporal Difference
 Learning als 
\begin_inset Quotes eld
\end_inset

erfahrene Belohnung
\begin_inset Quotes erd
\end_inset

 bezeichnet wurde, also hier 
\begin_inset Formula $r+\gamma\max_{a^{\prime}\in A}Q\left(s^{\prime},a^{\prime}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Da bei Neuronalen Netzen Training mit vielen Daten am Stück wesentlich effizient
er umgesetzt werden kann, empfiehlt es sich hier besonders mit einer fixierten
 Strategie Erfahrung zu sammeln um dann mit dieser zu trainieren.
\end_layout

\begin_layout Section
Fazit
\end_layout

\begin_layout Standard
Reinfoecement Learning stellt ein gutes Konzept dar, um Probleme zu lösen,
 welche sich als MDP formulieren lassen.
 Diese Klasse von Problemen mag im ersten Moment eher klein wirken, ist
 meiner Meinung nach jedoch viel größer als man denkt.
 Durch künstliche Erweiterung des State-Spaces (was durch Approximieren
 von Funktionen eher weniger ins Gewicht fällt) und zusätzlichen Variablen
 lässt sich Reinforcement Learning auch in Situationen verwenden, in denen
 die Zukunft entgegen der Markov-Eigenschaft sehr wohl von der Vergangenheit
 abhängt.
 Das Konzept der einen Aktion lässt sich ebenfalls sehr einfach auf mehrere
 parallele Aktionen transformieren, was das Verfahren noch universeller
 macht.
\end_layout

\begin_layout Standard
Reinforcement Learning stößt aber zum Beispiel dort an seine Grenzen, wo
 es keine natürlich definierten Belohnungen gibt und es Menschen sehr schwer
 fällt, geeignete und generalisierte Belohungen zu definieren.
 Auch in Fällen, in denen richtiges Handeln sehr verzögert belohnt wird,
 kann Reinforcement Learning versagen, da es zwar konvergieren könnte, dazu
 jedoch nicht hinnehmbare Mengen an Zeit benötigt.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Quellen
\end_layout

\begin_layout Description
AI
\begin_inset space ~
\end_inset

Course
\begin_inset space ~
\end_inset

CS188
\begin_inset space ~
\end_inset

(University
\begin_inset space ~
\end_inset

Berkley) 
\begin_inset CommandInset href
LatexCommand href
target "http://ai.berkeley.edu/home.html"

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset CommandInset href
LatexCommand href
target "https://www.youtube.com/channel/UCB4_W1V-KfwpTLxH9jG1_iA/videos"

\end_inset


\end_layout

\begin_layout Standard

\series bold
Harmon, Mance E.
 and Stephanie S.
 Harmon.
 “Reinforcement Learning: A Tutorial.” (1996).
\end_layout

\begin_layout Description
Q-Learning
\begin_inset space ~
\end_inset

(Code) 
\begin_inset CommandInset href
LatexCommand href
target "https://github.com/farizrahman4u/qlearning4k "

\end_inset


\end_layout

\begin_layout Description
SARSA
\begin_inset space ~
\end_inset

im
\begin_inset space ~
\end_inset

Detail 
\begin_inset CommandInset href
LatexCommand href
target "https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/"

\end_inset


\end_layout

\begin_layout Description
Double-Q-Learning 
\begin_inset CommandInset href
LatexCommand href
name "arXiv:1509.06461v3 [cs.LG]"
target "https://arxiv.org/abs/1509.06461"

\end_inset


\end_layout

\end_body
\end_document
